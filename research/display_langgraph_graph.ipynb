{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bf4f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msecurity\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mAuth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m authenticate_user, create_access_token\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomponents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mChatHistory\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mChatSessionDB\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MySQLDB\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomponents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mChatHistory\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mChatHistoryDB\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatHistoryDB\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import List, TypedDict, Annotated, Sequence, Optional\n",
    "\n",
    "import logging\n",
    "\n",
    "from IPython.core.display import Image\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage, AIMessageChunk\n",
    "from langchain_core.output_parsers import PydanticOutputParser, JsonOutputParser, StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# from langchain_community.vectorstores import Qdrant\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "# from langgraph.store.base import BaseStore\n",
    "\n",
    "from langgraph.graph import StateGraph, add_messages, END\n",
    "from pydantic import BaseModel\n",
    "from pydantic.v1 import Field\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "from langgraph.types import Command, interrupt\n",
    "from typing_extensions import Literal\n",
    "\n",
    "from redisvl.extensions.cache.llm import SemanticCache\n",
    "from psycopg import Connection\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "import redis\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.security.Auth import authenticate_user, create_access_token\n",
    "from src.components.ChatHistory.ChatSessionDB import MySQLDB\n",
    "from src.components.ChatHistory.ChatHistoryDB import ChatHistoryDB\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "###------------- Config Parser\n",
    "from configparser import ConfigParser\n",
    "\n",
    "parser = ConfigParser()\n",
    "config_file_path = 'config.properties'\n",
    "\n",
    "with open(config_file_path) as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "parser.read_string(file_content)\n",
    "###------------- Config Parser\n",
    "\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "\n",
    "#parser['CACHE']['cache_key']\n",
    "\n",
    "# from qdrant_client.http.models import Distance\n",
    "\n",
    "\n",
    "class DocumentLoader:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def document_reader(self) -> List[Document]:\n",
    "        loader = WebBaseLoader(self.url)\n",
    "        docs = loader.load()\n",
    "        return docs\n",
    "\n",
    "    def split_documents(self, documents):\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
    "        return splitter.split_documents(documents)\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, embedding_model, collection_name, qdrant_url, qdrant_key, vector_dimension):\n",
    "        self.embedding_model = embedding_model\n",
    "        # self.vector_db_client = QdrantClient(\":memory:\")\n",
    "        self.vector_db_client = QdrantClient(\n",
    "                url = qdrant_url,\n",
    "                api_key = qdrant_key\n",
    "            )\n",
    "        self.vector_store = None\n",
    "        self.collection_name = collection_name\n",
    "        self.vector_dimension = vector_dimension\n",
    "\n",
    "    def create_vectorstore(self):\n",
    "        # self.vector_db_client = QdrantClient(\":memory:\")\n",
    "        self.vector_db_client.create_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            vectors_config=VectorParams(size=self.vector_dimension, distance=Distance.COSINE),\n",
    "        )\n",
    "\n",
    "    def load_vector_store(self):\n",
    "        self.vector_store = QdrantVectorStore(\n",
    "            client=self.vector_db_client,\n",
    "            collection_name=self.collection_name,\n",
    "            embedding=self.embedding_model,\n",
    "        )\n",
    "        # self.vector_store.sim\n",
    "        # vectorstore.similarity_search_with_score(query, k=k)\n",
    "\n",
    "\n",
    "    def ingest_data(self, documents):\n",
    "        self.vector_store.add_documents(documents)\n",
    "\n",
    "    def check_if_collection_exists(self):\n",
    "        return self.vector_db_client.collection_exists(self.collection_name)\n",
    "\n",
    "    def get_retriever(self):\n",
    "        return self.vector_store.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\": 10})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be800ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict, Annotated, Sequence, Optional\n",
    "\n",
    "class RAGGraphState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    internal_raw_documents: list\n",
    "    relevant_documents: list\n",
    "    internal_retrieved_documents: str\n",
    "    generated_summary: str\n",
    "    query: str\n",
    "    final_response: dict\n",
    "    query_embedding: list\n",
    "    is_cached: bool\n",
    "    chat_history: str\n",
    "\n",
    "\n",
    "class RAGRetriever:\n",
    "    def __init__(self, vector_store, pg_connection_pool):\n",
    "        # self.retriever = retriever\n",
    "        # self.llm = ChatOpenAI(temperature=0, schemas = \"gpt-3.5-turbo\")\n",
    "        # self.llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "        # self.llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\")\n",
    "        # self.retrieve_documents_from_vector_db()\n",
    "        self.vector_store = vector_store\n",
    "        self.reranker_retriever = None\n",
    "        self.graph = self.generate_graph()\n",
    "\n",
    "        # self.redis_client = redis.Redis(host=parser['CACHE']['cache_url'], port=parser['CACHE']['cache_port'],\n",
    "        #                        db=parser['CACHE']['cache_db'], password=parser['CACHE']['cache_password'])\n",
    "\n",
    "        # self.llmcache = SemanticCache(\n",
    "        #     name=parser['CACHE']['cache_name'],\n",
    "        #     redis_client=self.redis_client,\n",
    "        #     distance_threshold=0.1\n",
    "        # )\n",
    "\n",
    "        # self.embedding = OpenAIEmbeddings(dimensions=768, model=\"text-embedding-3-small\")\n",
    "        self.pg_connection_pool = pg_connection_pool\n",
    "\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        embd_qry = self.embedding.embed_query(text)\n",
    "        return embd_qry\n",
    "\n",
    "    def test_get_pg_store(self, user_id):\n",
    "        pass\n",
    "        # pg_url = f\"postgresql://{parser['POSTGRES']['username']}:{parser['POSTGRES']['password']}@{parser['POSTGRES']['hostname']}:{parser['POSTGRES']['port']}/{parser['POSTGRES']['database']}?sslmode=disable\"\n",
    "        # with PostgresStore.from_conn_string(pg_url) as store:\n",
    "        #     namespace_for_memory = (user_id, \"memories\")\n",
    "        #     return store.get(namespace_for_memory, key=\"profile\")\n",
    "\n",
    "    def cache_checker(self, state: RAGGraphState, store) -> Command[Literal[\"final_node\", \"extract_profile_info\"]]:\n",
    "        query_text = state[\"query\"]\n",
    "        query_embedding = self.embed_query(query_text)\n",
    "\n",
    "        # Check cache first\n",
    "        cached_result = self.llmcache.check(vector=query_embedding)\n",
    "        # cached_result = None\n",
    "\n",
    "        if cached_result:\n",
    "            # Cache hit - return immediately\n",
    "            # print(f\"Cache HIT - Response time: {elapsed:.2f}s\")\n",
    "            logging.info(\"Cache hit - query: \" + query_text)\n",
    "            # response = cached_result[0]['metadata']\n",
    "            response = cached_result\n",
    "            logging.info(cached_result)\n",
    "            return Command(\n",
    "                goto='final_node',\n",
    "                update={\n",
    "                    \"final_response\": response,\n",
    "                    \"is_cached\": True\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            logging.info(\"Cache miss - query: \" + query_text)\n",
    "            return Command(\n",
    "                goto='extract_profile_info',\n",
    "                update={\n",
    "                    \"query\": state[\"query\"],\n",
    "                    \"query_embedding\": query_embedding,\n",
    "                    \"is_cached\": False\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def extract_profile_info(self, state: RAGGraphState, config: RunnableConfig, store) -> Command[Literal[\"vector_retriever\"]]:\n",
    "        config_user_id = config.get(\"configurable\").get(\"user_id\")\n",
    "        logging.info(\"config_user_id\")\n",
    "        logging.info(config_user_id)\n",
    "        # existing_memory = self.test_get_pg_store(config_user_id)\n",
    "\n",
    "        MEMORY_PROMPT_STRING = \"\"\"\n",
    "        You are an expert at extracting out profile information from conversations.\n",
    "        Extract personal information of the user, interests or anything that can help identify the user.\n",
    "        ** CONTEXT **\n",
    "        {context}\n",
    "        ** EXISTING CONVERSATIONS **\n",
    "        {existing_memory}\n",
    "        Do not return anything if nothing is found.\n",
    "        Consider the EXISTING CONVERSATIONS and do not create duplication profile information.\n",
    "        Strictly follow the following format for output : {format_instruction}\n",
    "        \"\"\"\n",
    "\n",
    "        jsonParser = JsonOutputParser()\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            template=MEMORY_PROMPT_STRING,\n",
    "            input_variables=[\"context\", \"existing_memory\"],\n",
    "            partial_variables={\"format_instruction\": jsonParser.get_format_instructions()}\n",
    "        )\n",
    "\n",
    "        chain = prompt | self.llm | jsonParser\n",
    "\n",
    "        # personal_info_result = chain.invoke({\"context\": state[\"query\"], \"existing_memory\": existing_memory})\n",
    "        # logging.info(\"personal_info_result\")\n",
    "        # logging.info(personal_info_result)\n",
    "\n",
    "        namespace_for_memory = (config_user_id, \"memories\")\n",
    "        # store.put(namespace_for_memory, \"profile\", personal_info_result)\n",
    "\n",
    "        return Command(\n",
    "            goto='vector_retriever',\n",
    "            update={\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def reranker(self, state: RAGGraphState, store) -> RAGGraphState:\n",
    "        model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
    "        compressor = CrossEncoderReranker(model=model, top_n=5)\n",
    "        # retriever = self.vector_store.as_retriever()\n",
    "        # docs = retriever.invoke(state[\"query\"])\n",
    "        # self.reranker_retriever = ContextualCompressionRetriever(\n",
    "        #     base_compressor=compressor, base_retriever=retriever\n",
    "        # )\n",
    "        return state\n",
    "\n",
    "    # def retrieve_documents_from_vector_db(self, state: RAGGraphState, store) -> Command[Literal[\"generate\"]]:\n",
    "    #     logging.info(\"retrieving related documents, query: \" + state[\"query\"])\n",
    "    #     retriever = self.vector_store.as_retriever()\n",
    "    #     docs = retriever.invoke(state[\"query\"])\n",
    "    #     context =  \"\\n\\n\".join([\"Content: \" + doc.page_content + \"\\n Source: \" + doc.metadata[\"source\"] for doc in docs])\n",
    "    #\n",
    "    #     return Command(\n",
    "    #         goto=\"generate\",\n",
    "    #         update={\n",
    "    #             \"internal_retrieved_documents\": context,\n",
    "    #         }\n",
    "    #     )\n",
    "\n",
    "    def retrieve_documents_from_vector_db(self, state: RAGGraphState, store) -> Command[Literal[\"rate_document_extraction\"]]:\n",
    "        logging.info(\"retrieving related documents, query: \" + state[\"query\"])\n",
    "        # retriever = self.vector_store.as_retriever()\n",
    "        # docs = retriever.invoke(state[\"query\"])\n",
    "        # context =  \"\\n\\n\".join([\"Content: \" + doc.page_content + \"\\n Source: \" + doc.metadata[\"source\"] for doc in docs])\n",
    "\n",
    "        return Command(\n",
    "            goto=\"rate_document_extraction\",\n",
    "            update={\n",
    "                \"internal_raw_documents\": [],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def rate_document_extraction(self, state: RAGGraphState, store) -> Command[Literal[\"generate\", \"web_search\"]]:\n",
    "        # reranked_docs = self.reranker_retriever.invoke({\"query\": query})\n",
    "        # return reranked_docs\n",
    "        prompt_template = (\n",
    "        \"You are a context relevance checker.\"\n",
    "        \"You will be given a question and chunk retrieved.\"\n",
    "        \"You need to return a relevance score between them in a range of 0 to 1 and short reason behind the relevance\"\n",
    "        \"Return a JSON response and follow the below response format for output.\"\n",
    "        \"{format_instructions}\"\n",
    "        \"Question: {question}\"\n",
    "        \"Chunk: {chunk}\"\n",
    "        )\n",
    "\n",
    "        class RelevanceScore(BaseModel):\n",
    "            score: float\n",
    "            reason: str\n",
    "\n",
    "        outputParser = PydanticOutputParser(pydantic_object=RelevanceScore)\n",
    "\n",
    "        prompt = PromptTemplate(template=prompt_template,\n",
    "                                input_variables=[\"question\", \"chunk\"],\n",
    "                                partial_variables={\"format_instructions\": outputParser.get_format_instructions()}\n",
    "                                )\n",
    "        # prompt = PromptTemplate.from\n",
    "\n",
    "        docs = state[\"internal_raw_documents\"]\n",
    "\n",
    "        chain = prompt | self.llm | outputParser\n",
    "\n",
    "        relevant_docs = []\n",
    "        relevant_doc_flag = False\n",
    "        for doc in docs:\n",
    "            output = chain.invoke({\"question\": state[\"query\"],\n",
    "                                   \"chunk\": doc.page_content\n",
    "                                   })\n",
    "            logging.info(\"Comparing\")\n",
    "            logging.info(state[\"query\"])\n",
    "            logging.info(doc.page_content)\n",
    "            logging.info(output)\n",
    "            if output.score > 0.8:\n",
    "                relevant_doc_flag = True\n",
    "            if output.score>0.5:\n",
    "                relevant_docs.append(doc)\n",
    "\n",
    "        # context = \"\\n\\n\".join([\"Content: \" + doc.page_content + \"\\n Source: \" + doc.metadata[\"source\"] for doc in relevant_docs])\n",
    "        # context = \"\\n\\n\".join(\n",
    "        #     [\"Content: \" + doc.page_content + \"\\n Source: \" + doc.metadata[\"source\"] for doc in docs])\n",
    "        refined_relevant_results = [{\"page_content\": doc.page_content, \"metadata\": {\"source\": doc.metadata[\"source\"]}} for doc in relevant_docs]\n",
    "        if relevant_doc_flag:\n",
    "            goto = \"generate\"\n",
    "        else:\n",
    "            goto = \"web_search\"\n",
    "        return Command(\n",
    "            goto=goto,\n",
    "            update={\n",
    "                \"relevant_documents\": refined_relevant_results\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def web_search(self, state: RAGGraphState, store) -> Command[Literal[\"generate\"]]:\n",
    "\n",
    "        web_search_tool = TavilySearchResults(max_results = 3)\n",
    "\n",
    "        web_results = web_search_tool.invoke({\"query\": state[\"query\"]})\n",
    "        refined_results = [{\"page_content\": doc[\"content\"], \"metadata\": {\"source\": doc[\"url\"]}} for doc in web_results]\n",
    "\n",
    "        updated_relevant_docs = state[\"relevant_documents\"] + refined_results\n",
    "\n",
    "        goto = \"generate\"\n",
    "        return Command(\n",
    "            goto = goto,\n",
    "            update={\n",
    "                \"relevant_documents\": updated_relevant_docs\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def generate(self, state: RAGGraphState, store) -> Command[Literal[\"final_node\"]]:\n",
    "        GENERATE_PROMPT = (\n",
    "            \"You are an assistant for question-answering tasks.\\n\"\n",
    "            \"Use the following pieces of retrieved context to answer the question.\\n\"\n",
    "            \"If the answer cannot be found from the context, just say that you don't know, ask if you can help with other questions and do not generate responses on your own.\\n\"\n",
    "            \"Keep the answer short and concise.\\n\"\n",
    "            \"Existing conversations: {chat_history}\"\n",
    "            \"Question: {question} \\n\"\n",
    "            \"Context: {context}\"\n",
    "            \n",
    "            \"Return the answer first and then the corresponding sources of the documents behind the answers in new lines\" \n",
    "            \"Follow the format instruction for the output:\"\n",
    "            \"<ANSWER>\"\n",
    "            \"\\n\"\n",
    "            \"Sources: <source_1>\"\n",
    "            \"         ..  \"\n",
    "            \"         <source_n>\"\n",
    "        )\n",
    "        # logging.info(\"Generating response, query: \" + state[\"query\"])\n",
    "        # prompt = GENERATE_PROMPT.format(question=state[\"query\"], chat_history=state[\"chat_history\"],\n",
    "        #                                 context=state[\"internal_retrieved_documents\"])\n",
    "        # response = self.llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "        # class ResponseFormat(BaseModel):\n",
    "        #     answer: str = Field(description=\"Answer of the question\")\n",
    "        #     source: Optional[list[str]] = Field(description=\"List of sources of the retrieved data\")\n",
    "\n",
    "        # parser = PydanticOutputParser(pydantic_object=ResponseFormat)\n",
    "        parser = StrOutputParser()\n",
    "\n",
    "        # prompt = PromptTemplate(\n",
    "        #     template=GENERATE_PROMPT,\n",
    "        #     input_variables=[\"chat_history\", \"question\", \"context\"],\n",
    "        #     partial_variables={\"format_instruction\": parser.get_format_instructions()}\n",
    "        # )\n",
    "        prompt = PromptTemplate(\n",
    "            template=GENERATE_PROMPT,\n",
    "            input_variables=[\"chat_history\", \"question\", \"context\"]\n",
    "        )\n",
    "        docs = state[\"relevant_documents\"]\n",
    "        # for doc in docs:\n",
    "        #     logging.info(\"DOCUMENT - \")\n",
    "        #     logging.info(doc)\n",
    "        #     logging.info(doc[\"page_content\"])\n",
    "        #     logging.info(doc[\"metadata\"][\"source\"])\n",
    "\n",
    "        # context =  \"\\n\\n\".join([\"Content: \" + doc.page_content + \"\\n Source: \" + doc.metadata['source'] for doc in docs])\n",
    "        # refined_relevant_results = [{\"page_content\": doc.page_content, \"metadata\": {\"source\": doc.metadata[\"source\"]}}\n",
    "        #                             for doc in relevant_docs]\n",
    "        context = \"\\n\\n\".join([\"Content: \" + doc[\"page_content\"] + \"\\n Source: \" + doc[\"metadata\"][\"source\"] for doc in docs])\n",
    "\n",
    "        chain = prompt | self.llm | parser\n",
    "        response = chain.invoke({\"question\":state[\"query\"],\n",
    "                                 \"chat_history\":state[\"chat_history\"],\n",
    "                                 \"context\":context})\n",
    "        # self.reranker_retriever = ContextualCompressionRetriever(\n",
    "        #     base_compressor=compressor, base_retriever=retriever\n",
    "        # dict_response = response.model_dump()\n",
    "        # )\n",
    "        # logging.info(\"Generated response, query: \" + state[\"query\"])\n",
    "        # logging.info(\"RESPONSE\")\n",
    "        # logging.info(type(response))\n",
    "        # logging.info(response)\n",
    "        # logging.info(\"JSON RESPONSE\")\n",
    "        # logging.info(type(dict_response))\n",
    "        # logging.info(dict_response)\n",
    "        return Command(\n",
    "            goto=\"final_node\",\n",
    "            update={\n",
    "                \"final_response\": response\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def final_node(self, state: RAGGraphState, store) -> Command[Literal[END]]:\n",
    "\n",
    "        if not state[\"is_cached\"]:\n",
    "            # self.llmcache.store(\n",
    "            #     prompt=state[\"query\"],\n",
    "            #     response=state[\"final_response\"][\"answer\"],\n",
    "            #     metadata=state[\"final_response\"],\n",
    "            #     vector=state[\"query_embedding\"]\n",
    "            # )\n",
    "            self.llmcache.store(\n",
    "                prompt=state[\"query\"],\n",
    "                response=state[\"final_response\"],\n",
    "                vector=state[\"query_embedding\"]\n",
    "            )\n",
    "            logging.info(\"Cached answer\")\n",
    "        logging.info(\"Generated final answer, query: \" + state[\"query\"])\n",
    "        return Command(\n",
    "            goto=END,\n",
    "            update={\n",
    "                \"final_response\": state[\"final_response\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def generate_graph(self):\n",
    "        logging.info(\"generate_graph\")\n",
    "        graph = StateGraph(RAGGraphState)\n",
    "        # graph.add_node(\"reranker\", self.reranker)\n",
    "        graph.add_node(\"cache_checker\", self.cache_checker)\n",
    "        graph.add_node(\"extract_profile_info\", self.extract_profile_info)\n",
    "        graph.add_node(\"vector_retriever\", self.retrieve_documents_from_vector_db)\n",
    "        graph.add_node(\"rate_document_extraction\", self.rate_document_extraction)\n",
    "        graph.add_node(\"web_search\", self.web_search)\n",
    "        graph.add_node(\"generate\", self.generate)\n",
    "        graph.add_node(\"final_node\", self.final_node)\n",
    "        # graph.add_edge(\"vector_retriever\", \"generate\")\n",
    "        graph.set_entry_point(\"cache_checker\")\n",
    "        graph.set_finish_point(\"final_node\")\n",
    "        # pg_url = f\"postgresql://{parser['POSTGRES']['username']}:{parser['POSTGRES']['password']}@{parser['POSTGRES']['hostname']}:{parser['POSTGRES']['port']}/{parser['POSTGRES']['database']}?sslmode=disable\"\n",
    "        # memory_connection_kwargs = {\n",
    "        #     \"autocommit\": True,\n",
    "        #     \"prepare_threshold\": 0\n",
    "        # }\n",
    "        # logging.info(\"About to connect\")\n",
    "        # pool = Connection.connect(pg_url)\n",
    "        # logging.info(\"Connected\")\n",
    "        # checkpointer = PostgresSaver(pool)\n",
    "        # with PostgresSaver.from_conn_string(pg_url) as checkpointer:\n",
    "        # with PostgresSaver(pool) as checkpointer:\n",
    "        graph = graph.compile()\n",
    "        # Image(graph.get_graph().draw_mermaid_png())\n",
    "        display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "        return graph\n",
    "\n",
    "    async def query(self, query, chat_history):\n",
    "        # logging.info(\"STREAM DATA\")\n",
    "        # logging.info(\"Query answer\")\n",
    "        # logging.info(response)\n",
    "        pg_url = f\"postgresql://{parser['POSTGRES']['username']}:{parser['POSTGRES']['password']}@{parser['POSTGRES']['hostname']}:{parser['POSTGRES']['port']}/{parser['POSTGRES']['database']}?sslmode=disable\"\n",
    "\n",
    "        connection_kwargs = {\n",
    "            \"autocommit\": True,\n",
    "            \"prepare_threshold\": 0,\n",
    "        }\n",
    "\n",
    "        conn = Connection.connect(pg_url, **connection_kwargs)\n",
    "        store = PostgresStore(conn)\n",
    "        # store.setup()\n",
    "\n",
    "        checkpointer = PostgresSaver(conn)\n",
    "        # checkpointer = InMemoryStore()\n",
    "        # checkpointer.setup()\n",
    "\n",
    "        # graph = self.graph.compile(checkpointer=checkpointer, store = store)\n",
    "        graph = self.graph.compile(store=store)\n",
    "        config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"user_123\"}}\n",
    "        # response = graph.invoke({\"query\": query, \"chat_history\": chat_history}, config)\n",
    "        # return response[\"final_response\"]\n",
    "\n",
    "        async for event in graph.astream_events(input={\"query\": query, \"chat_history\": chat_history}, config=config, version=\"v2\"):\n",
    "        # async for event in graph.astream_events(input={\"query\": query, \"chat_history\": chat_history}, version=\"v2\"):\n",
    "        #     logging.info(\"Streaming... -> \")\n",
    "        #     logging.info(event)\n",
    "            yield event\n",
    "        # return graph.astream_events(input={\"query\": query, \"chat_history\": chat_history}, config=config, version=\"v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "class RAGApplication:\n",
    "    def __init__(self):\n",
    "\n",
    "        # self.vector_manager = VectorStore(embedding_model = OpenAIEmbeddings(   dimensions=int(parser['EMBEDDING']['dimensions']),\n",
    "        #                                                                         model=parser['EMBEDDING']['model']),\n",
    "        #                                                                         collection_name = parser['QDRANT']['collection_name'],\n",
    "        #                                                                         qdrant_url = parser['QDRANT']['url'],\n",
    "        #                                                                         qdrant_key = parser['QDRANT']['api_key'],\n",
    "                                                                                # vector_dimension=int(parser['QDRANT']['vector_dimension']))\n",
    "\n",
    "        self.rag_chain = None\n",
    "\n",
    "    def ingest_data(self, docs_path: str):\n",
    "        print(\"Ingesting documents...\")\n",
    "        if self.rag_chain is None:\n",
    "            self.load_store()\n",
    "\n",
    "        doc_loader = DocumentLoader(url=docs_path)\n",
    "        docs = doc_loader.document_reader()\n",
    "        split_docs = doc_loader.split_documents(docs)\n",
    "        self.vector_manager.ingest_data(split_docs)\n",
    "\n",
    "    def load_store(self):\n",
    "        logging.info(\"Loading vector store\")\n",
    "        # self.vector_manager.load_vector_store()\n",
    "        # if not self.vector_manager.check_if_collection_exists():\n",
    "        #     logging.info(\"Vector store is not created, hence creating\")\n",
    "        #     self.vector_manager.create_vectorstore()\n",
    "\n",
    "        # self.vector_manager.load_vector_store()\n",
    "        logging.info(\"loaded vector store\")\n",
    "        self.rag_chain = RAGRetriever(vector_store=None, pg_connection_pool = None)\n",
    "\n",
    "    async def answer_question(self, question: str, chat_history: str) -> str:\n",
    "        if self.rag_chain is None:\n",
    "            self.load_store()\n",
    "        async for event in self.rag_chain.query(question, chat_history):\n",
    "            yield event\n",
    "\n",
    "\n",
    "    async def stream_answer_question(self, question: str, chat_history: str) -> str:\n",
    "        if self.rag_chain is None:\n",
    "            self.load_store()\n",
    "        async for event in self.rag_chain.query(question, chat_history):\n",
    "            logging.info(event)\n",
    "            # if 'metadata' in event and 'langgraph_node' in event['metadata']:\n",
    "            if 'metadata' in event:\n",
    "                node_name = \"\"\n",
    "                if \"langgraph_node\" in event['metadata']:\n",
    "                    node_name = event['metadata'][\"langgraph_node\"]\n",
    "\n",
    "                process_description = \"\"\n",
    "                if node_name == \"cache_checker\":\n",
    "                    process_description = \"Checking Cache\"\n",
    "                elif node_name == \"extract_profile_info\":\n",
    "                    process_description = \"Extracting profile information\"\n",
    "                elif node_name == \"vector_retriever\":\n",
    "                    process_description = \"Retrieving from vector DB\"\n",
    "                elif node_name == \"rate_document_extraction\":\n",
    "                    process_description = \"Analysing extracted documents\"\n",
    "                elif node_name == \"web_search\":\n",
    "                    process_description = \"Searching over web\"\n",
    "                elif node_name == \"generate\":\n",
    "                    process_description = \"Generating response\"\n",
    "                elif node_name == \"final_node\":\n",
    "                    process_description = \"Finished Generation\"\n",
    "\n",
    "                # if (\"data\" in event and \"chunk\" in event[\"data\"]):\n",
    "                #     logging.info(\"Type\")\n",
    "                #     logging.info(type(event[\"data\"][\"chunk\"]))\n",
    "                #     logging.info(isinstance(event[\"data\"][\"chunk\"], AIMessageChunk))\n",
    "                #     logging.info(isinstance(event[\"data\"][\"chunk\"], dict))\n",
    "\n",
    "                content = \"\"\n",
    "                if (\"data\" in event and \"chunk\" in event[\"data\"] and isinstance(event[\"data\"][\"chunk\"], AIMessageChunk)):\n",
    "                    content = event[\"data\"][\"chunk\"].content\n",
    "\n",
    "                is_cached = False\n",
    "                if (\"data\" in event and \"output\" in event[\"data\"] and isinstance(event[\"data\"][\"output\"], dict)):\n",
    "                    is_cached = event[\"data\"][\"output\"].get(\"is_cached\", False)\n",
    "\n",
    "                final_response = \"\"\n",
    "                if (\"data\" in event and \"output\" in event[\"data\"] and isinstance(event[\"data\"][\"output\"], dict) and \"final_response\" in event[\"data\"][\"output\"] and isinstance(event[\"data\"][\"output\"][\"final_response\"], list) and len(event[\"data\"][\"output\"][\"final_response\"])>0):\n",
    "                    final_response = event['data']['output']['final_response'][-1].get('response')\n",
    "\n",
    "                filtered_event = {\"event\": event[\"event\"],\n",
    "                                  \"name\": event[\"name\"],\n",
    "                                  \"content\": content,\n",
    "                                  \"node_name\": node_name,\n",
    "                                  \"process_description\": process_description,\n",
    "                                  \"is_cached\":is_cached,\n",
    "                                  \"final_response\": final_response\n",
    "                                  }\n",
    "                yield filtered_event\n",
    "            # if event[\"event\"] == \"on_chat_model_stream\":\n",
    "            #     filtered_event = {\"event\": event[\"event\"],\n",
    "            #                       \"name\": event[\"name\"],\n",
    "            #                       \"data\": event[\"data\"],\n",
    "            #                       \"content\": event[\"data\"][\"chunk\"].content\n",
    "            #                       }\n",
    "            #     yield filtered_event\n",
    "                # yield event[\"data\"][\"chunk\"].content\n",
    "\n",
    "            # elif event[\"event\"] == \"on_chain_end\" and ((isinstance(event[\"data\"][\"output\"], dict) and event[\"data\"][\"output\"].get(\"final_response\") is not None)):\n",
    "            #     filtered_event = {\"event\": event[\"event\"],\n",
    "            #                       \"data\": {\n",
    "            #                           \"output\": {\n",
    "            #                                     \"final_response\": event[\"data\"][\"output\"][\"final_response\"]\n",
    "            #                             } if (isinstance(event[\"data\"][\"output\"], dict) and event[\"data\"][\"output\"].get(\"final_response\") is not None) else {}\n",
    "            #                       }\n",
    "            #                       }\n",
    "            #     yield filtered_event\n",
    "                # yield event[\"data\"][\"output\"][\"final_response\"]\n",
    "        #\n",
    "        # return async_response\n",
    "        # return self.rag_chain.query(question, chat_history)\n",
    "\n",
    "\n",
    "class ChatHistoryHandler:\n",
    "    def __init__(self):\n",
    "        self.chatHistoryMongoDB = ChatHistoryDB(username = parser['MONGODB']['mongodb_username'],\n",
    "                                          password = parser['MONGODB']['mongodb_password'],\n",
    "                                          hostname = parser['MONGODB']['mongodb_hostname'],\n",
    "                                          database = parser['MONGODB']['mongodb_database'],\n",
    "                                          keyspace = parser['MONGODB']['mongodb_keyspace'],\n",
    "                                          port = parser['MONGODB']['mongodb_port']\n",
    "                                          )\n",
    "\n",
    "    def retrive_chat(self, username, session_id, record_limit):\n",
    "        results = self.chatHistoryMongoDB.get_record(username=username, session_id=session_id, record_limit=record_limit)\n",
    "        return results\n",
    "\n",
    "    def insert_chat_record(self, message, username, session_id, timestamp, role):\n",
    "        data = {\n",
    "            \"key\": username + \"_\" + session_id + \"_\" + timestamp,\n",
    "            \"data\": message,\n",
    "            \"username\": username,\n",
    "            \"session_id\": session_id,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"role\": role\n",
    "        }\n",
    "        self.chatHistoryMongoDB.insert_record(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1087c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatSessionListHandler:\n",
    "    def __init__(self):\n",
    "        self.chatSessionMySQL = MySQLDB(host = parser[\"MYSQL\"][\"mysql_hostname\"],\n",
    "                                        port = parser[\"MYSQL\"][\"mysql_port\"],\n",
    "                                        username = parser[\"MYSQL\"][\"mysql_username\"],\n",
    "                                        password = parser[\"MYSQL\"][\"mysql_password\"],\n",
    "                                        database = parser[\"MYSQL\"][\"mysql_database\"]\n",
    "                                        )\n",
    "\n",
    "    def retrive_chat(self, username):\n",
    "        results = self.chatSessionMySQL.get_chat_sessions_by_username(username=username)\n",
    "        return results\n",
    "\n",
    "    def insert_chat_record(self, session_id, username, description, timestamp):\n",
    "        self.chatSessionMySQL.insert_new_session(\n",
    "            session_id = session_id,\n",
    "            username = username,\n",
    "            description = description,\n",
    "            timestamp = timestamp\n",
    "        )\n",
    "\n",
    "    '''\n",
    "    Authenticate user\n",
    "    '''\n",
    "    def authenticate(self, username, password):\n",
    "        response = authenticate_user(username, password, self.chatSessionMySQL)\n",
    "        if response:\n",
    "            logging.info(\"main - authenticate()\")\n",
    "            logging.info(response)\n",
    "            data = {\"sub\": username}\n",
    "            token = create_access_token(data, parser['ENCRYPTION']['SECRET_KEY'], parser['ENCRYPTION']['ALGORITHM'],\n",
    "                                        int(parser['ENCRYPTION']['ACCESS_TOKEN_EXPIRE_MINUTES']))\n",
    "            return token\n",
    "        return response\n",
    "\n",
    "    def update_stock_token_controller(self, token, username):\n",
    "        logging.info(\"Calling update_stock_token_controller()\")\n",
    "        logging.info(token)\n",
    "        current_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        self.chatSessionMySQL.update_stock_token(token, current_time, username)\n",
    "        return {\"message\": \"Successfully Updated the stock token\"}\n",
    "\n",
    "    def get_stock_token_controller(self, username):\n",
    "        return self.chatSessionMySQL.get_basic_user_info(username)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed3ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def query_data(query):\n",
    "    # query = input(\"question - \")\n",
    "    # query = \"Which is the highest peak in Northeast?\"\n",
    "    # query = \"Which is the highest peak in Northeast?\"\n",
    "    # query = \"What is the longest river in Earth\"\n",
    "    # async_response =\n",
    "    # logging.info(async_response)\n",
    "    logging.info(\"Steaming... -> -> -> -> ->\")\n",
    "    # async for event in rag_app.answer_question(question=query, chat_history=\"\"):\n",
    "    async for event in rag_app.stream_answer_question(question=query, chat_history=\"\"):\n",
    "        # logging.info(\"10101010---------\")\n",
    "        logging.info(event)\n",
    "\n",
    "    logging.info(\"Completed -----------\")\n",
    "    # answer = await rag_app.answer_question(question=query, chat_history=\"\")\n",
    "    # print(\"\\nAnswer:\", answer, \"\\n\")\n",
    "    # return answer\n",
    "    return \"Sample Response\"\n",
    "\n",
    "async def config_pg_checkpointer():\n",
    "    pg_url = f\"postgresql://{parser['POSTGRES']['username']}:{parser['POSTGRES']['password']}@{parser['POSTGRES']['hostname']}:{parser['POSTGRES']['port']}/{parser['POSTGRES']['database']}?sslmode=disable\"\n",
    "    # memory_connection_kwargs = {\n",
    "    #     \"autocommit\": True,\n",
    "    #     \"prepare_threshold\": 0\n",
    "    # }\n",
    "    # logging.info(\"About to connect\")\n",
    "    # pool = Connection.connect(pg_url)\n",
    "    # logging.info(\"Connected\")\n",
    "    # checkpointer = PostgresSaver(pool)\n",
    "    # await checkpointer.setup()\n",
    "    # logging.info(\"Setup done\")\n",
    "    with PostgresSaver.from_conn_string(pg_url) as checkpointer:\n",
    "        # Your graph logic goes here\n",
    "        # The checkpointer will be available within this block\n",
    "        # Call setup() once to create tables\n",
    "        checkpointer.setup()\n",
    "        await asyncio.sleep(5)\n",
    "        logging.info(\"Setup done\")\n",
    "\n",
    "async def config_pg_store():\n",
    "    pg_url = f\"postgresql://{parser['POSTGRES']['username']}:{parser['POSTGRES']['password']}@{parser['POSTGRES']['hostname']}:{parser['POSTGRES']['port']}/{parser['POSTGRES']['database']}?sslmode=disable\"\n",
    "    connection_kwargs = {\n",
    "        \"autocommit\": True,\n",
    "        \"prepare_threshold\": 0,\n",
    "    }\n",
    "    conn = Connection.connect(pg_url, **connection_kwargs)\n",
    "    store = PostgresStore(conn)\n",
    "    store.setup()\n",
    "    await asyncio.sleep(5)\n",
    "    logging.info(\"Setup done\")\n",
    "\n",
    "def test_load_pg_store():\n",
    "    pg_url = f\"postgresql://{parser['POSTGRES']['username']}:{parser['POSTGRES']['password']}@{parser['POSTGRES']['hostname']}:{parser['POSTGRES']['port']}/{parser['POSTGRES']['database']}?sslmode=disable\"\n",
    "    connection_kwargs = {\n",
    "        \"autocommit\": True,\n",
    "        \"prepare_threshold\": 0,\n",
    "    }\n",
    "    conn = Connection.connect(pg_url, **connection_kwargs)\n",
    "    store = PostgresStore(conn)\n",
    "\n",
    "    logging.info(\"About to insert\")\n",
    "    user_id = \"123\"\n",
    "    namespace_for_memory = (user_id, \"memories\")\n",
    "    # response = store.get(namespace_for_memory, key='profile')\n",
    "    profile = {'user': {'name': 'Avik Roy'}, 'interests': {'resgion': 'North East', 'topic': 'main food'}}\n",
    "    store.put(namespace_for_memory, \"profile\", profile)\n",
    "\n",
    "    logging.info(\"Inserted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b99d1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_app = RAGApplication()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04634c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_app.load_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    docs_path = \"https://en.wikipedia.org/wiki/Northeast_India\"\n",
    "\n",
    "    rag_app = RAGApplication()\n",
    "    query = \"What was the cricket score in india vs Namibia?\"\n",
    "    # answer = rag_app.answer_question(question=query, chat_history=\"\")\n",
    "    # chatSessionListHandler = ChatSessionListHandler()\n",
    "    # RAGRetriever()\n",
    "    # username = \"us\"\n",
    "    # password = \"ps\"\n",
    "    # response = chatSessionListHandler.authenticate(username, password)\n",
    "    x = 1\n",
    "    #\n",
    "    # rag_app.ingest_data(docs_path)\n",
    "    # rag_app.load_store()\n",
    "    #\n",
    "    # pg_url = f\"postgresql://{parser['POSTGRES']['username']}:{parser['POSTGRES']['password']}@{parser['POSTGRES']['hostname']}:{parser['POSTGRES']['port']}/{parser['POSTGRES']['database']}?sslmode=disable\"\n",
    "    # # logging.info(pg_url)\n",
    "    # memory_connection_kwargs = {\n",
    "    #     \"autocommit\": True,\n",
    "    #     \"prepare_threshold\": 0\n",
    "    # }\n",
    "    # asyncio.run(config_pg_checkpointer())\n",
    "    # asyncio.run(config_pg_store())\n",
    "    # test_load_pg_store()\n",
    "\n",
    "    # username = \"us\"\n",
    "    # password = \"ps\"\n",
    "    # chatSessionListHandler.authenticate(username=username, password=password)\n",
    "\n",
    "    # logging.info(\"About to connect\")\n",
    "    # pool = Connection.connect(pg_url)\n",
    "    # logging.info(\"Connected\")\n",
    "    # checkpointer = PostgresSaver(pool)\n",
    "    # checkpointer.setup()\n",
    "    # query = \"What was the cricket score in india vs Namibia?\"\n",
    "    query = \"Major mountains peaks in the North East?\"\n",
    "    # query = \"What are the major travelling destinations in North East?\"\n",
    "    # asyncio.run(query_data(query))\n",
    "    # while True:\n",
    "    #     # query = input(\"question - \")\n",
    "    #     query = \"Which is the highest peak in Northeast?\"\n",
    "    #     # query = \"What is the longest river in Earth\"\n",
    "    #     if query.lower() in [\"exit\", \"quit\"]:\n",
    "    #         break\n",
    "        # answer = rag_app.answer_question(question=query, chat_history=\"\")\n",
    "        # logging.info(\"\\nAnswer:\", answer, \"\\n\")\n",
    "\n",
    "    # embedding = OpenAIEmbeddings(dimensions=768, model=\"text-embedding-3-small\")\n",
    "    #\n",
    "    # from redisvl.extensions.cache.llm import SemanticCache\n",
    "\n",
    "    # llmcache = SemanticCache(\n",
    "    #     name=\"llmcache\",\n",
    "    #     vectorizer=hf,  # Your HuggingFace vectorizer\n",
    "    #     redis_url=REDIS_URL,\n",
    "    #     ttl=300,  # Cache entries expire after 5 minutes\n",
    "    #     distance_threshold=0.2,  # Similarity threshold\n",
    "    #     overwrite=True\n",
    "    # )\n",
    "    # redis_client = redis.Redis(host=parser['CACHE']['cache_url'], port=parser['CACHE']['cache_port'],\n",
    "    #                            db=parser['CACHE']['cache_db'], password=parser['CACHE']['cache_password'])\n",
    "    #\n",
    "    # llmcache = SemanticCache(\n",
    "    #     name=\"llmcache\",  # The name of the search index in Redis\n",
    "    #     # redis_url=\"redis://103.180.212.180:6379\",  # Connection URL for Redis\n",
    "    #     redis_client=redis_client,\n",
    "    #     distance_threshold=0.1  # Similarity threshold for cache matching\n",
    "    # )\n",
    "\n",
    "    # def answer_question_with_cache(query_text):\n",
    "    #     start_time = time.time()\n",
    "    #\n",
    "    #     # Embed the query\n",
    "    #     query_vector = embed_query(query_text)\n",
    "    #\n",
    "    #     # Check cache first\n",
    "    #     cached_result = llmcache.check(vector=query_vector)\n",
    "    #\n",
    "    #     if cached_result:\n",
    "    #         # Cache hit - return immediately\n",
    "    #         elapsed = time.time() - start_time\n",
    "    #         print(f\"Cache HIT - Response time: {elapsed:.2f}s\")\n",
    "    #         return cached_result[0]['response']\n",
    "    #\n",
    "    #     # Cache miss - run full RAG pipeline\n",
    "    #     # answer = await answer_question(index, query_text)\n",
    "    #     answer = \"Here is the sample result for the query - \" + query_text\n",
    "    #\n",
    "    #     # Store in cache for future queries\n",
    "    #     llmcache.store(\n",
    "    #         prompt=query_text,\n",
    "    #         response=answer,\n",
    "    #         vector=query_vector\n",
    "    #     )\n",
    "    #\n",
    "    #     elapsed = time.time() - start_time\n",
    "    #     print(f\"Cache MISS - Response time: {elapsed:.2f}s\")\n",
    "    #     return answer\n",
    "\n",
    "    # query = \"Which is the longest river in the world?\"\n",
    "    # embds.append(embed_query(query))\n",
    "    # query = \"What is the longest river in Earth\"\n",
    "    # query = \"Tell me the name of the longest river in the world.\"\n",
    "    # embds.append(embed_query(query))\n",
    "    # x = 1\n",
    "    # answer = answer_question_with_cache(query)\n",
    "    # print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgraph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
